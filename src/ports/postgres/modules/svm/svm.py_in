from __future__ import division

import plpy

from utilities.control import MinWarning
from utilities.in_mem_group_control import GroupIterationController
from utilities.validate_args import explicit_bool_to_text
from utilities.utilities import unique_string
from utilities.utilities import extract_keyvalue_params
from utilities.utilities import add_postfix
from utilities.utilities import _string_to_array_with_quotes
from utilities.utilities import _string_to_array
from utilities.utilities import _assert
from utilities.utilities import num_features

from utilities.validate_args import cols_in_tbl_valid
from utilities.validate_args import input_tbl_valid
from utilities.validate_args import output_tbl_valid
from utilities.validate_args import is_var_valid
from utilities.validate_args import get_expr_type

from validation.internal.cross_validation import CrossValidator
import numpy as np


def _compute_svm(args):
    """
    Compute SVM coefficients

    @return Number of iterations that has been run
    """
    init_stepsize = args['init_stepsize']
    args['stepsize'] = init_stepsize
    iterationCtrl = GroupIterationController(args)
    with iterationCtrl as it:
        it.iteration = 0
        has_converged = False
        while not has_converged:
            it.update(
                """
                {schema_madlib}.linear_svm_igd_step(
                    ({col_ind_var})::FLOAT8[],
                    ({col_dep_var_trans})::FLOAT8,
                    {rel_state}.{col_grp_state},
                    {n_features}::INT4,
                    {stepsize}::FLOAT8,
                    {lambda}::FLOAT8,
                    {is_l2}::BOOLEAN,
                    {col_n_tuples},
                    ({select_epsilon})::FLOAT8,
                    {is_svc}::BOOLEAN
                    )
                """)
            it.info()
            if it.kwargs['decay_factor'] > 0:
                it.kwargs['stepsize'] *= it.kwargs['decay_factor']
            else:
                it.kwargs['stepsize'] = init_stepsize / (it.iteration + 1)
            has_converged = it.test(
                """
                {iteration} >= {max_iter}
                OR {schema_madlib}.internal_linear_svm_igd_distance(
                    _state_previous, _state_current) < {tolerance}
                """)
        it.final()
    return iterationCtrl.iteration
# ---------------------------------------------------


def _verify_table(source_table, model_table, dependent_varname,
                  independent_varname, **kwargs):
    # validate input
    input_tbl_valid(source_table, 'SVM')
    _assert(is_var_valid(source_table, dependent_varname),
            "SVM error: invalid dependent_varname "
            "('{dependent_varname}') for source_table "
            "({source_table})!".format(dependent_varname=dependent_varname,
                                       source_table=source_table))
    _assert(is_var_valid(source_table, independent_varname),
            "SVM error: invalid independent_varname "
            "('{independent_varname}') for source_table "
            "({source_table})!".format(independent_varname=independent_varname,
                                       source_table=source_table))

    dep_type = get_expr_type(dependent_varname, source_table)
    if '[]' in dep_type:
        plpy.error("SVM error: dependent_varname cannot be of array type!")

    # validate output tables
    output_tbl_valid(model_table, 'SVM')
    summary_table = add_postfix(model_table, "_summary")
    output_tbl_valid(summary_table, 'SVM')


def _verify_grouping(schema_madlib, source_table, grouping_col):
    if grouping_col and grouping_col.lower() != 'null':
        cols_in_tbl_valid(source_table,
                          _string_to_array_with_quotes(grouping_col),
                          'SVM')
        intersect = frozenset(
            _string_to_array(grouping_col)).intersection(
                frozenset(
                    ('coef', 'random_feature_data',
                     'random_feature_data', 'loss'
                     'num_rows_processed', 'num_rows_skipped',
                     'norm_of_gradient', 'num_iterations')))
        _assert(len(intersect) == 0,
                "SVM error: Conflicting grouping column name.\n"
                "Some predefined keyword(s) ({0}) are not allowed "
                "for grouping column names!".format(', '.join(intersect)))

        grouping_list = [i + "::text"
                         for i in explicit_bool_to_text(
                             source_table,
                             _string_to_array_with_quotes(grouping_col),
                             schema_madlib)]
        grouping_str = ','.join(grouping_list)
    else:
        grouping_str = "Null"
        grouping_col = None

    return grouping_str, grouping_col


def _verify_params_dict(params_dict):
    _assert(not hasattr(params_dict['lambda'], '__len__'),
            "SVM Error: lambda should not be a list after cross validation!")
    _assert(not hasattr(params_dict['epsilon'], '__len__'),
            "SVM Error: epsilon should not be a list after cross validation!")
    _assert(not hasattr(params_dict['init_stepsize'], '__len__'),
            "SVM Error: init_stepsize should not be a "
            "list after cross validation!")
    _assert(not hasattr(params_dict['decay_factor'], '__len__'),
            "SVM Error: decay_factor should not be a "
            "list after cross validation!")
    _assert(not hasattr(params_dict['max_iter'], '__len__'),
            "SVM Error: max_iter should not be a list after cross validation!")
    return params_dict


def _build_output_tables(n_iters_run, model_table, args, transformer,**kwargs):
    if transformer is None:
        dependent_varname = args['col_dep_var']
        independent_varname = args['col_ind_var']
        source_table = args['rel_source']
        transformer_str = "NULL"
        kernel_func = "NULL"
        kernel_params = "NULL"
    else:
        original_table = transformer.original_table
        dependent_varname = original_table['dependent_varname']
        independent_varname = original_table['independent_varname']
        source_table = original_table['source_table']
        transformer_str = transformer.saveAsText()
        kernel_func = transformer.kernel_func
        kernel_params = transformer.kernel_params

    grouping_col = args['grouping_col']
    col_grp_key = args['col_grp_key']
    groupby_str, grouping_str1, using_str = "", "", "ON TRUE"
    if grouping_col:
        groupby_str = "GROUP BY {grouping_col}, {col_grp_key}".format(
            grouping_col=grouping_col, col_grp_key=col_grp_key)
        grouping_str1 = grouping_col + ","
        using_str = "USING ({col_grp_key})".format(col_grp_key=col_grp_key)
    # organizing results
    dep_type = get_expr_type(dependent_varname, source_table)
    model_table_query = """
        CREATE TABLE {model_table} AS
            SELECT
                {grouping_str1}
                (result).coefficients           AS coef,
                (result).loss                   AS loss,
                (result).norm_of_gradient       AS norm_of_gradient,
                {n_iters_run}                   AS num_iterations,
                (result).num_rows_processed     AS num_rows_processed,
                n_tuples_including_nulls - (result).num_rows_processed
                                                AS num_rows_skipped,
                '{transformer}'::text
                                                AS random_feature_data,
                ARRAY[{mapping}]::{dep_type}[]  AS dep_var_mapping
            FROM
            (
                SELECT
                    {schema_madlib}.internal_linear_svm_igd_result(
                        {col_grp_state}
                    ) AS result,
                    {col_grp_key}
                FROM {rel_state}
                WHERE {col_grp_iteration} = {n_iters_run}
            ) rel_state_subq
            JOIN
            (
                SELECT
                    {grouping_str1}
                    count(*) AS n_tuples_including_nulls,
                    array_to_string(ARRAY[{grouping_str}],
                                    ','
                                   ) AS {col_grp_key}
                FROM {source_table}
                {groupby_str}
            ) n_tuples_including_nulls_subq
            {using_str}
        """.format(n_iters_run=n_iters_run,
                   groupby_str=groupby_str,
                   grouping_str1=grouping_str1,
                   using_str=using_str,
                   source_table=source_table,
                   model_table=model_table,
                   transformer=transformer_str,
                   dep_type=dep_type, **args)
    plpy.execute(model_table_query)

    args['lambda_str'] = str(args['lambda'])
    summary_table = add_postfix(model_table, "_summary")
    grouping_text = "NULL" if not grouping_col else grouping_col
    plpy.execute("""
            CREATE TABLE {summary_table} AS
            SELECT
                '{method}'::text                    AS method,
                '__MADLIB_VERSION__'::text          AS version_number,
                '{source_table}'::text              AS source_table,
                '{model_table}'::text               AS model_table,
                '{dependent_varname}'::text         AS dependent_varname,
                '{independent_varname}'::text       AS independent_varname,
                '{kernel_func}'::text               AS kernel_func,
                '{kernel_params}'::text             AS kernel_params,
                '{grouping_text}'::text             AS grouping_col,
                'init_stepsize={init_stepsize}, '   ||
                    'decay_factor={decay_factor}, ' ||
                    'max_iter={max_iter}, '         ||
                    'tolerance={tolerance}'::text   AS optim_params,
                'lambda={lambda_str}, ' ||
                    'norm={norm}, '     ||
                    'n_folds={n_folds}'::text       AS reg_params,
                count(*)::integer                   AS num_all_groups,
                0::integer                          AS num_failed_groups,
                sum(num_rows_processed)::bigint     AS total_rows_processed,
                sum(num_rows_skipped)::bigint       AS total_rows_skipped,
                '{epsilon}'::double precision       AS epsilon,
                '{eps_table}'::text                 AS eps_table
            FROM {model_table};
            """.format(grouping_text=grouping_text,
                       summary_table=summary_table,
                       source_table=source_table,
                       model_table=model_table,
                       kernel_func=kernel_func,
                       kernel_params=kernel_params,
                       dependent_varname=dependent_varname,
                       independent_varname=independent_varname,
                       **args))


def svm(schema_madlib, source_table, model_table,
        dependent_varname, independent_varname, kernel_func,
        kernel_params, grouping_col, params, is_svc,
        verbose, **kwargs):
    """
    Executes the linear support vector classification algorithm.
    """
    # verbosing
    verbosity_level = "info" if verbose else "error"
    with MinWarning(verbosity_level):
        _verify_table(source_table,
                      model_table,
                      dependent_varname,
                      independent_varname)
        grouping_str, grouping_col = _verify_grouping(schema_madlib,
                              source_table, grouping_col)
        kernel_func = _verify_kernel(kernel_func)
        transformer = _random_feature_map(schema_madlib, source_table,
                            dependent_varname, independent_varname,
                            kernel_func, kernel_params, grouping_col)
        params_dict = _extract_params(schema_madlib, params)
        args = locals()
        if transformer is not None:
            args.update(transformer.transformed_table)
        _cross_validate_svm(args)
        _svm_parsed_params(**args)


def _cross_validate_svm(args):
    # updating params_dict will also update args['params_dict']
    params_dict = args['params_dict']

    if params_dict['n_folds'] > 1 and args['grouping_col']:
        plpy.error('SVM Error: cross validation '
                   'with grouping is not supported!')

    cv_params = {}
    if len(params_dict['lambda']) > 1:
        cv_params['lambda'] = params_dict['lambda']
    else:
        params_dict['lambda'] = params_dict['lambda'][0]
    if len(params_dict['epsilon']) > 1 and not args['is_svc']:
        cv_params['epsilon'] = params_dict['epsilon']
    else:
        params_dict['epsilon'] = params_dict['epsilon'][0]
    if len(params_dict['init_stepsize']) > 1:
        cv_params['init_stepsize'] = params_dict['init_stepsize']
    else:
        params_dict['init_stepsize'] = params_dict['init_stepsize'][0]
    if len(params_dict['max_iter']) > 1:
        cv_params['max_iter'] = params_dict['max_iter']
    else:
        params_dict['max_iter'] = params_dict['max_iter'][0]
    if len(params_dict['decay_factor']) > 1:
        cv_params['decay_factor'] = params_dict['decay_factor']
    else:
        params_dict['decay_factor'] = params_dict['decay_factor'][0]

    if not cv_params and params_dict['n_folds'] <= 1:
        # no cross validation
        return

    if cv_params and params_dict['n_folds'] <= 1:
        plpy.error("SVM Error: All parameters must be scalar "
                   "or of length 1 when n_folds is 0 or 1")

    if not cv_params and params_dict['n_folds'] > 1:
        plpy.warning('SVM Warning: n_folds > 1 but no cross validate params provided'
                     'Ignoring cross validation request.')
        return

    scorer = 'classification' if args['is_svc'] else 'regression'
    sub_args = {'params_dict': cv_params}
    transformer = args.get('transformer', None)
    # we want svm in cross validation to behave as if transformer is None
    # if it is not, then svm_predict will transform the test data again,
    # which will not be correct since test data in cross validation
    # comes from training data which has already been transformed
    args.update(dict(transformer=None))
    cv = CrossValidator(_svm_parsed_params, svm_predict, scorer, args)
    val_res = cv.validate(sub_args, params_dict['n_folds']).sorted()
    val_res.output_tbl(params_dict['validation_result'])
    params_dict.update(val_res.first('sub_args')['params_dict'])
    args.update(dict(transformer=transformer))
# ------------------------------------------------------------------------------


class GaussianKernel(object):
    """docstring for gaussianKernel"""
    def __init__(self, gamma=1, n_components=100, random_state=1,
                 random_weights=None, random_offset=None):
        self.kernel_func = 'gaussian'
        self.gamma = gamma
        self.n_components = n_components
        self.random_state = random_state
        self.rd_weights = random_weights
        self.rd_offset = random_offset
        self.transformed_table = dict()
        self.original_table = dict()
        if self.rd_offset is not None:
            self.n_components = num_features(self.rd_offset, 'val')

    def saveAsText(self):
        return ','.join([str(self.gamma), self.rd_weights, self.rd_offset])

    @classmethod
    def loadFromText(cls, text):
        gamma, rd_weights, rd_offset = text.split(',')
        return cls(gamma=float(gamma),
                   random_weights=rd_weights,
                   random_offset=rd_offset)

    @property
    def kernel_params(self):
        return ('gamma={gamma}, n_components={n_components}'
                .format(gamma=self.gamma,
                        n_components=self.n_components))

    def fit(self, n_features, kernel_params=''):

        def _create_mat(X, tbl_name, row='id', val='val'):
            header = [row, val]
            header_str = ','.join(header)
            header_with_type_str = ','.join([row + ' integer',
                                            val + ' float[]'])
            plpy.execute("""
                         DROP TABLE IF EXISTS {tbl_name};
                         CREATE TABLE {tbl_name} ({header})
                         """.format(tbl_name=tbl_name,
                                    header=header_with_type_str))
            data = []
            for i, r in enumerate(X, 1):
                data.append("({id}, '{{{val}}}')"
                            .format(id=i, val=','.join(map(str, r))))
            data = ','.join(data)
            plpy.execute("""
                         INSERT INTO {tbl_name}({header}) VALUES
                         {data}""".format(data=data,
                                          header=header_str,
                                          tbl_name=tbl_name))
            return tbl_name

        params_default = {
            'gamma': self.gamma,
            'n_components': self.n_components}

        params_types = {
            'gamma': float,
            'n_components': int}

        params_vals = extract_keyvalue_params(kernel_params,
                                              params_types,
                                              params_default)

        self.gamma = params_vals['gamma']
        self.n_components = params_vals['n_components']

        random_state = np.random.RandomState(self.random_state)
        random_weights_ = (np.sqrt(2 * self.gamma) * random_state.normal(
            size=(n_features, self.n_components)))
        random_offset_ = random_state.uniform(0, 2 * np.pi,
                                              size=self.n_components)
        # augment it to a 1 * n_components matrix
        random_offset_ = random_offset_[np.newaxis, :]

        self.rd_weights = _create_mat(random_weights_,
                                      unique_string(desp='random_weights'))
        self.rd_offset = _create_mat(random_offset_,
                                     unique_string(desp='random_offsets'))

    def clear_tables(self):
        plpy.execute("drop table if exists {0};".format(self.rd_weights))
        plpy.execute("drop table if exists {0};".format(self.rd_offset))

    def transform(self, schema_madlib, source_table, independent_varname,
                  dependent_varname=None, grouping_col=None, id_col=None,
                  transformed_name='gaussian_transformed'):
        if not self.rd_offset or not self.rd_weights:
            return self

        self.original_table = dict(source_table=source_table,
                                   independent_varname=independent_varname,
                                   dependent_varname=dependent_varname)

        grouping_col = ("NULL::integer as grouping_col"
                        if not grouping_col
                        else "{0}").format(grouping_col)

        dependent_varname = ("NULL::integer"
                             if not dependent_varname
                             else "{0}").format(dependent_varname)

        id_col = ("NULL::integer as id_col"
                  if not id_col
                  else "{0}").format(id_col)

        # copy data to the temporary table with id column
        # id_col is different from index_col
        # id_col is unique and, if any, is from the original table
        # index_col is generated randomly
        # needs to be sequential for madlib.matrix_mult to work
        source_with_id = unique_string(desp='source_copied')
        features_col = unique_string(desp='features_col')
        target_col = unique_string(desp='target_col')
        index_col = unique_string(desp='index_col')
        run_sql = """
            select setseed(0.5);
            drop table if exists {source_with_id};
            create temp table {source_with_id} as
                select
                    row_number() over (order by random()) as {index_col},
                    {dependent_varname} as {target_col},
                    {independent_varname} as {features_col},
                    {id_col},
                    {grouping_col}
                from {source_table}
        """.format(source_table=source_table,
                   source_with_id=source_with_id,
                   id_col=id_col,
                   index_col=index_col,
                   dependent_varname=dependent_varname,
                   independent_varname=independent_varname,
                   grouping_col=grouping_col,
                   target_col=target_col, features_col=features_col)
        plpy.execute(run_sql)
        source_table = source_with_id
        dependent_varname = target_col
        independent_varname = features_col

        temp_transformed = unique_string(desp='temp_transformed')
        # X = X * weights
        run_sql = """
            drop table if exists {temp_transformed};
            select {schema_madlib}.matrix_mult(
                          '{source_table}',
                          'row={index_col}, val={independent_varname}',
                          '{rd_weights}',
                          'row=id, val=val',
                          '{temp_transformed}',
                          'row={index_col}, val={independent_varname}');
        """.format(temp_transformed=temp_transformed,
                   schema_madlib=schema_madlib,
                   source_table=source_table,
                   rd_weights=self.rd_weights,
                   index_col=index_col,
                   independent_varname=independent_varname)
        plpy.execute(run_sql)

        transformed = unique_string(desp=transformed_name)

        # X = a * cos (X + b)
        multiplier = np.sqrt(2.) / np.sqrt(self.n_components)
        run_sql = """
            drop table if exists {transformed};
            create temp table {transformed} as
                select
                    {index_col},
                    {schema_madlib}.array_scalar_cos(
                            q.{independent_varname}::float[],
                            {multiplier}::float) as {independent_varname},
                    {dependent_varname},
                    {id_col},
                    {grouping_col}
                from (
                    select
                        x.{index_col},
                        {schema_madlib}.array_add(
                            x.{independent_varname}::float[],
                            o.val::float[]) as {independent_varname}
                    from {temp_transformed} as x cross join {rd_offset} as o
                ) q join {source_table} s using ({index_col})
        """.format(index_col=index_col,
                   id_col=id_col,
                   dependent_varname=dependent_varname,
                   schema_madlib=schema_madlib,
                   independent_varname=independent_varname,
                   multiplier=multiplier,
                   grouping_col=grouping_col,
                   transformed=transformed,
                   source_table=source_table,
                   temp_transformed=temp_transformed,
                   rd_offset=self.rd_offset)
        plpy.execute(run_sql)
        # clear table generated from matrix mult
        plpy.execute("drop table {0};".format(temp_transformed))
        self.transformed_table = dict(index_col=index_col,
                                      source_table=transformed,
                                      dependent_varname=dependent_varname,
                                      independent_varname=independent_varname)
        return self


def _verify_kernel(kernel_func):
    kernel_func = 'linear' if not kernel_func else kernel_func.lower()
    # Add non-linear kernels below after implementing them.
    supported_kernels = ['linear', 'gaussian']
    try:
        # allow user to specify a prefix substring of
        # supported kernel function names. This works because the supported
        # kernel functions have unique prefixes.
        kernel_func = next(x for x in supported_kernels
                           if x.startswith(kernel_func))
    except StopIteration:
        # next() returns a StopIteration if no element found
        plpy.error("SVM Error: Invalid kernel function: "
                   "{0}. Supported kernel functions are ({1})"
                   .format(kernel_func, ','.join(sorted(supported_kernels))))
    return kernel_func


def _random_feature_map(schema_madlib, source_table, dependent_varname,
                        independent_varname, kernel_func,
                        kernel_params, grouping_col):
    if kernel_func == 'linear':
        return None
    elif kernel_func == 'gaussian':
        n_features = num_features(source_table, independent_varname)
        transformer = GaussianKernel()
        transformer.fit(n_features, kernel_params)
        return transformer.transform(schema_madlib, source_table,
                    independent_varname, dependent_varname, grouping_col)



def _svm_parsed_params(schema_madlib, source_table, model_table,
                       dependent_varname, independent_varname, transformer,
                       grouping_str, grouping_col, params_dict, is_svc,
                       verbose, **kwargs):
    """
    Executes the linear support vector algorithm.
    """
    n_features = num_features(source_table, independent_varname)

    args = {
        'rel_args': unique_string(desp='rel_args'),
        'rel_state': unique_string(desp='rel_state'),
        'col_grp_iteration': unique_string(desp='col_grp_iteration'),
        'col_grp_state': unique_string(desp='col_grp_state'),
        'col_grp_key': unique_string(desp='col_grp_key'),
        'col_n_tuples': unique_string(desp='col_n_tuples'),
        'state_type': "double precision[]",
        'n_features': n_features,
        'verbose': verbose,
        'is_svc': is_svc,
        'schema_madlib': schema_madlib,
        'grouping_str': grouping_str,
        'grouping_col': grouping_col,
        'rel_source': source_table,
        'col_ind_var': independent_varname,
        'col_dep_var': dependent_varname}

    args.update(_verify_params_dict(params_dict))
    args.update(_process_epsilon(is_svc, args))
    args.update(_svc_or_svr(is_svc, source_table, dependent_varname))

    # place holder for compatibility
    plpy.execute("CREATE TABLE pg_temp.{0} AS SELECT 1"
                 .format(args['rel_args']))
    # actual iterative algorithm computation
    n_iters_run = _compute_svm(args)
    _build_output_tables(n_iters_run, model_table,
                         args, transformer, **kwargs)


def svm_predict(schema_madlib, model_table, new_data_table, id_col_name,
                output_table, **kwargs):
    """ Scores the data points stored in a table using a
        learned support vector model.

    @param model_table Name of learned model
    @param new_data_table Name of table/view containing the data
        points to be scored
    @param id_col_name Name of column in source_table containing
        (integer) identifier for data point
    @param output_table Name of table to store the results
    """
    # suppress warnings
    with MinWarning("warning"):
        # model table
        input_tbl_valid(model_table, 'SVM')
        cols_in_tbl_valid(model_table, ['coef'], 'SVM')
        # summary table
        summary_table = add_postfix(model_table, "_summary")
        input_tbl_valid(summary_table, 'SVM')
        cols_in_tbl_valid(summary_table,
                          ['dependent_varname', 'independent_varname',
                           'kernel_func', 'kernel_params', 'grouping_col'],
                          'SVM')

        # read necessary info from summary
        summary = plpy.execute("""
                SELECT
                    method,
                    dependent_varname,
                    independent_varname,
                    kernel_func,
                    kernel_params,
                    grouping_col
                FROM {summary_table}
                """.format(**locals()))[0]
        method = summary['method']
        dependent_varname = summary['dependent_varname']
        independent_varname = summary['independent_varname']
        kernel_func = summary['kernel_func']
        kernel_params = summary['kernel_params']
        grouping_col = summary['grouping_col']
        grouping_col = None if grouping_col == 'NULL' else grouping_col

        input_tbl_valid(new_data_table, 'SVM')
        grouping_str, grouping_col = _verify_grouping(schema_madlib,
                                                      new_data_table,
                                                      grouping_col)
        _assert(is_var_valid(new_data_table, independent_varname),
                "SVM Error: independent_varname ('" + independent_varname +
                "') is invalid for new_data_table (" + new_data_table + ")!")
        _assert(id_col_name is not None, "SVM Error: id_col_name is NULL!")
        _assert(is_var_valid(new_data_table, id_col_name),
                "SVM Error: id_col_name ('" + id_col_name +
                "') is invalid for new_data_table (" + new_data_table + ")!")
        output_tbl_valid(output_table, 'SVM')

        if kernel_func == 'gaussian':
            model = plpy.execute("""
                        select random_feature_data from {0}
                        """.format(model_table))[0]
            transformer = GaussianKernel.loadFromText(
                                model['random_feature_data'])
            transformer.transform(schema_madlib, new_data_table,
                                  independent_varname,
                                  grouping_col=grouping_col,
                                  id_col=id_col_name)
            transformed_table = transformer.transformed_table
            new_data_table = transformed_table['source_table']
            independent_varname = transformed_table['independent_varname']
            dependent_varname = transformed_table['dependent_varname']

        if method.upper() == 'SVC':
            pred_query = """
                        CASE WHEN {schema_madlib}.array_dot(
                                    coef::double precision [],
                                    {independent_varname}::double precision []
                                ) >= 0
                            THEN dep_var_mapping[2]
                            ELSE dep_var_mapping[1]
                        END
                        """.format(schema_madlib=schema_madlib,
                                   independent_varname=independent_varname)
        elif method.upper() == 'SVR':
            pred_query = """
                        {schema_madlib}.array_dot(
                                coef::double precision [],
                                {independent_varname}::double precision [])
                        """.format(schema_madlib=schema_madlib,
                                   independent_varname=independent_varname)
        else:
            plpy.error("SVM Error: Invalid 'method' value in summary table. "
                       "'method' can only be SVC or SVR!")

        if grouping_col:
            sql = """
            CREATE TABLE {output_table} AS
            SELECT
                {id_col_name} AS {id_col_name},
                {pred_query} AS prediction,
                ARRAY[{grouping_str}] as grouping_col,
                {grouping_col}
            FROM {model_table}
            JOIN {new_data_table}
            USING ({grouping_col})
            WHERE not {schema_madlib}.array_contains_null({independent_varname})
            ORDER BY grouping_col, {id_col_name}
            """.format(**locals())
        else:
            sql = """
            CREATE TABLE {output_table} AS
            SELECT
                {id_col_name} AS {id_col_name},
                {pred_query} as prediction
            FROM
                {model_table},
                {new_data_table}
            WHERE
                not {schema_madlib}.array_contains_null({independent_varname})
            """.format(**locals())
        plpy.execute(sql)


def _svc_or_svr(is_svc, source_table, dependent_varname):
    # transform col_dep_var to binary (1`or -1) if classification
    _args = {'col_dep_var_trans': dependent_varname,
             'mapping': 'NULL',
             'method': 'SVR'}

    if is_svc:
        # dependent variable mapping
        dep_labels = plpy.execute("""
            SELECT {dependent_varname} AS y
            FROM {source_table}
            WHERE ({dependent_varname}) IS NOT NULL
            GROUP BY ({dependent_varname})
            ORDER BY ({dependent_varname})
            """.format(source_table=source_table,
                       dependent_varname=dependent_varname))

        dep_var_mapping = ["'{0}'".format(d['y'])
                           if isinstance(d['y'], basestring)
                           else str(d['y']) for d in dep_labels]

        _assert(len(dep_var_mapping) == 2,
                "SVM Error: Classification currently "
                "only supports binary output!")

        col_dep_var_trans = (
            """
            CASE WHEN ({col_dep_var}) IS NULL THEN NULL
                WHEN ({col_dep_var}) = {mapped_value_for_negative} THEN -1.0
                ELSE 1.0
            END
            """
            .format(col_dep_var=dependent_varname,
                    mapped_value_for_negative=dep_var_mapping[0])
            )

        _args.update({
            'mapped_value_for_negative': dep_var_mapping[0],
            'col_dep_var_trans': col_dep_var_trans,
            'mapping': dep_var_mapping[0] + "," + dep_var_mapping[1],
            'method': 'SVC'})

    return _args


def _process_epsilon(is_svc, args):
    eps_table = args['eps_table']
    grouping_col = args['grouping_col']
    grouping_str = args['grouping_str']
    col_grp_key = args['col_grp_key']
    rel_source = args['rel_source']
    epsilon = args['epsilon']
    rel_epsilon = ''
    select_epsilon = '{0}'.format(epsilon)
    as_rel_source = '_src'

    if not is_svc and grouping_col and eps_table:
        rel_epsilon = unique_string(desp='rel_epsilon')
        input_tbl_valid(eps_table, 'SVM')
        _assert(is_var_valid(eps_table, grouping_col),
                "SVM Error: invalid column names ('{grouping_col}') "
                "for eps_table ('{eps_table}')!"
                .format(grouping_col=grouping_col,
                        eps_table=eps_table))
        plpy.execute("""
            DROP TABLE IF EXISTS {rel_epsilon};
            CREATE TEMPORARY TABLE {rel_epsilon} AS (
                    SELECT
                        {col_grp_key},
                        coalesce(epsilon, {epsilon}) AS epsilon
                    FROM (
                        SELECT
                            array_to_string(ARRAY[{grouping_str}], ',') AS
                                {col_grp_key}
                        FROM
                            {rel_source}
                        GROUP BY {grouping_col}
                    ) q1
                    LEFT JOIN
                    (
                        SELECT
                            array_to_string(ARRAY[{grouping_str}], ',') AS
                                {col_grp_key},
                               epsilon
                        FROM {eps_table}
                    ) q2
                    USING ({col_grp_key})
            );
            """.format(rel_epsilon=rel_epsilon,
                       col_grp_key=col_grp_key,
                       epsilon=epsilon,
                       grouping_str=grouping_str,
                       rel_source=rel_source,
                       grouping_col=grouping_col,
                       eps_table=eps_table))

        select_epsilon = (
            """
            (
                SELECT epsilon
                FROM
                    {rel_epsilon}
                WHERE
                    {rel_epsilon}.{col_grp_key} = {as_rel_source}.{col_grp_key}
            )
            """
            .format(rel_epsilon=rel_epsilon,
                    as_rel_source=as_rel_source,
                    col_grp_key=col_grp_key))

    return {'select_epsilon': select_epsilon,
            'epsilon': epsilon,
            'rel_epsilon': rel_epsilon,
            'as_rel_source': as_rel_source}


def _extract_params(schema_madlib, params, module='SVM'):
    # NOTICE: the type of values in params_default should be consistent with
    # the types specified in params_types
    params_default = {
        'init_stepsize': [0.01],
        'decay_factor': [0.9],
        'max_iter': [100],
        'tolerance': 1e-10,
        'lambda': [0.01],
        'norm': 'L2',
        'n_folds': 0,
        'validation_result': '',
        'epsilon': [0.01],
        'eps_table': ''}

    params_types = {
        'init_stepsize': list,
        'decay_factor': list,
        'max_iter': list,
        'tolerance': float,
        'lambda': list,
        'norm': str,
        'n_folds': int,
        'validation_result': str,
        'epsilon': list,
        'eps_table': str}

    params_vals = extract_keyvalue_params(params,
                                          params_types,
                                          params_default)
    if params_vals['n_folds'] < 0:
        plpy.error("{0} Error: n_folds must be non-negative!".format(module))

    # validate lambda
    params_vals['lambda'] = map(float, params_vals['lambda'])
    _assert(all(lmd >= 0 for lmd in params_vals['lambda']),
            "{0} Error: lambda must be non-negative!".format(module))
    # validate epsilon
    params_vals['epsilon'] = map(float, params_vals['epsilon'])
    _assert(all(e >= 0 for e in params_vals['epsilon']),
            "{0} Error: epsilon must be non-negative!".format(module))
    # validating cross validation is delegated to _cross_validate_svm()
    params_vals['init_stepsize'] = map(float, params_vals['init_stepsize'])
    _assert(all(e > 0 for e in params_vals['init_stepsize']),
            "{0} Error: init_stepsize must be positive!".format(module))
    params_vals['max_iter'] = map(int, params_vals['max_iter'])
    _assert(all(e > 0 for e in params_vals['max_iter']),
            "{0} Error: max_iter must be positive!".format(module))
    params_vals['decay_factor'] = map(float, params_vals['decay_factor'])
    _assert(all(e <= 1 for e in params_vals['decay_factor']),
            "{0} Error: decay_factor must be <= 1!".format(module))

    if params_vals['validation_result']:
        output_tbl_valid(params_vals['validation_result'], 'SVM')

    params_vals['norm'] = params_vals['norm'].lower()
    _assert(params_vals['norm'] == 'l1' or params_vals['norm'] == 'l2',
            "{0} Error: norm must be either L1 or L2!".format(module))
    _assert(params_vals['tolerance'] >= 0,
            "{0} error: tolerance must be non-negative!".format(module))

    params_vals['is_l2'] = True if params_vals['norm'] == 'l2' else False
    return params_vals
